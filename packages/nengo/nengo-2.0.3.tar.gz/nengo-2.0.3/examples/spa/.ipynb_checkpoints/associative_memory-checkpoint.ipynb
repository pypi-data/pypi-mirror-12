{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associative Memory\n",
    "\n",
    "This tutorial introduces the Associative Memory (AM) module in the SPA.\n",
    "\n",
    "An associative memory is a neural network used to store and recall patterns.\n",
    "When the network receives a partial or noisy pattern at the input, it can either recover the same pattern or recall another stored pattern. \n",
    "If the recovered pattern is the same as the input pattern, the memory is said to be autoassociative or a \"clean-up\" memory. Otherwise, if the recovered pattern is different from the presented one, the network is heteroassociative. \n",
    "\n",
    "Patterns stored by the AM module in the SPA are semantic pointers organised in a SPA vocabulary.\n",
    "The examples in this tutorial demonstrate how to use the AM module, store and recall patterns.\n",
    "Advanced functionality of the module, such as the recall of multiple memories similar to the input, is also presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# python libraries and plotting tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nengo\n",
    "from nengo import spa\n",
    "%load_ext nengo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating an Associative Memory\n",
    "\n",
    "We start by creating a set of patterns the AM will store. \n",
    "The vocabulary in this example contains five words: `ORANGE, APRICOT, CHERRY, STRAWBERRY` and `APPLE`.\n",
    "Each word is represented as a semantic pointer, an $n$-dimensional vector.\n",
    "When creating a vocabulary, we specify the number of dimensions for all semantic pointers.\n",
    "Then, we add the words to a vocabulary with `parse`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 32\n",
    "vocab = spa.Vocabulary(dimensions=dim)\n",
    "\n",
    "words = ['ORANGE', 'APRICOT', 'CHERRY', 'STRAWBERRY', 'APPLE']\n",
    "\n",
    "for word in words:\n",
    "    vocab.parse(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an autoassociative memory with a corresponding set of stored patterns.\n",
    "The memory is created within a `nengo.Network` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with nengo.Network('AssociativeMemory', seed=1) as model1:\n",
    "    assoc_mem = spa.AssociativeMemory(input_vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the network has been created, we can test its functionality by presenting a semantic pointer as input and comparing the output to a semantic pointer. The comparison is done by calculating the dot product. If the similarity between the output vector and the input vector is close to one, we can say that the associative memory successfully retrieved the pattern (or cleaned up the input).\n",
    "To achieve this task, we need to introduce two additional objects: `nengo.Node` and `nengo.Probe`. The `nengo.Node` object provides input to the associative memory. \n",
    "`nengo.Probe` is used to gather simulation data, in our case the input being fed to the network (input pattern) and the network output (output pattern) at every simulation time step.\n",
    "In this example, the input to the network is the semantic pointer `APPLE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with model1:\n",
    "    input_vector = vocab['APPLE'].v\n",
    "    input_node = nengo.Node(output=input_vector, label='input')\n",
    "    \n",
    "    nengo.Connection(input_node, assoc_mem.input)\n",
    "\n",
    "    input_probe = nengo.Probe(input_node)\n",
    "    output_probe = nengo.Probe(assoc_mem.output, synapse=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the simulation for the 0.2 seconds simulation time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim = nengo.Simulator(model1)\n",
    "sim.run(0.2)\n",
    "t = sim.trange()\n",
    "print('Number of simulation steps: %d' % len(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the probes to get the data produced by the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_net = sim.data[input_probe]\n",
    "output_net = sim.data[output_probe]\n",
    "print(\"Input dimensions: (%d, %d)\" % input_net.shape)\n",
    "print(\"Output dimensions: (%d, %d)\" % output_net.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for each simulation step (rows of the `input_net` and the `output_net` arrays) there is a 64-dimensional pattern represented in the associative memory. \n",
    "We use a dot product (implemented in `nengo.spa.similarity`) to compare the similarity of inputs and outputs to all patterns in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_similarities(input_net, output_net, vocab1, vocab2=None, top=None):\n",
    "    if vocab2 is None:\n",
    "        vocab2 = vocab1\n",
    "    autoscale=False\n",
    "    \n",
    "    if top is None:\n",
    "        autoscale=True\n",
    "        \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.autoscale(autoscale, axis='y')\n",
    "    if not autoscale:\n",
    "        plt.ylim(top=top)\n",
    "    plt.plot(t, spa.similarity(input_net, vocab1))\n",
    "    plt.title(\"Input similarity\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.legend(vocab1.keys, loc='best')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(t, spa.similarity(output_net, vocab2))\n",
    "    plt.title(\"Output similarity\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.legend(vocab2.keys, loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_similarities(input_net, output_net, vocab, top=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see that the network input has been constant throughout the simulation (`APPLE=1`). Notice that there might be some small similarity between the pattern `APPLE` and some other patterns. This is because the semantic pointers are not perfectly orthogonal and the dot product will amount to some value different from zero. This can be improved by increasing the dimensionality of vectors, yielding more orthogonal representations.\n",
    "At the output, the similarity of the represented semantic pointer with the semantic pointer `APPLE` increases until it reaches the maximal value (`=1`). This means that the associative memory successfully retrieved a pattern similar to the input pattern. The exponential increase is due to the synaptic filtering in `nengo.Probe`. This is used to show how the input to another group of neurons connected to the output of this particular AM module would look like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean-up Memory\n",
    "\n",
    "\n",
    "In the next example, we show the ability of the associative memory to retrieve a clean pattern from a noisy input. \n",
    "This is a very common use of autoassociative memories, also called \"clean-up\" as the AM removes the noise in the input.\n",
    "\n",
    "Now, instead of presenting just one semantic pointer from a vocabulary, we present a combination of several semantic pointers. This is still one single vector, which is mostly similar to one semantic pointer but is also somewhat similar to other semantic pointers. Therefore, as the input is not \"clean\" we can say that we are presenting a noisy pattern.\n",
    "The effect would have been similar if we had chosen randomly distributed numbers using, for example, `numpy`'s `random.rand` function. To indicate that the output of the AM should be similar to just one stored vector, we set the parameter `wta_output` to `True`. `WTA` is a computational principle called winner-take-all, stating that one, mostly active element should be regarded as the winner among possible, less similar alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with nengo.Network('Cleanup', seed=1) as model2:\n",
    "    assoc_mem = spa.AssociativeMemory(input_vocab=vocab, wta_output=True)\n",
    "    \n",
    "    input_vector = 0.2*vocab['APPLE'].v + 0.7*vocab['CHERRY'].v + 0.5*vocab['APRICOT'].v\n",
    "    input_node = nengo.Node(output=input_vector, label='input')\n",
    "    nengo.Connection(input_node, assoc_mem.input)\n",
    "\n",
    "    input_probe = nengo.Probe(input_node)\n",
    "    output_probe = nengo.Probe(assoc_mem.output, synapse=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim = nengo.Simulator(model2)\n",
    "sim.run(0.2)\n",
    "t = sim.trange()\n",
    "\n",
    "input_net = sim.data[input_probe]\n",
    "output_net = sim.data[output_probe]\n",
    "\n",
    "plot_similarities(input_net, output_net, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the input presented to the network is mostly similar to the semantic pointer `CHERRY`, but it is also similar to other semantic pointers in the vocabulary (e.g. `APRICOT` and `APPLE`). At the output, the network successfully represents a pattern similar to `CHERRY`. If we wanted to get a pattern at the output whose similarity with the most similar vocabulary item is one, we could have used the `threshold_output` parameter when creating the AM module and setting it to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful feature of the AM module is thresholding of the inputs. By specifying a threshold, the output will be similar only to those inputs whose similarity with a vocabulary item is above a certain value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with nengo.Network('CleanupThreshold', seed=1) as model3:\n",
    "    assoc_mem = spa.AssociativeMemory(input_vocab=vocab, threshold=0.4)\n",
    "    \n",
    "    input_vector = vocab['APPLE'].v + 0.8*vocab['CHERRY'].v + 0.3*vocab['APRICOT'].v\n",
    "    input_node = nengo.Node(output=input_vector, label='input')\n",
    "    nengo.Connection(input_node, assoc_mem.input)\n",
    "\n",
    "    input_probe = nengo.Probe(input_node)\n",
    "    output_probe = nengo.Probe(assoc_mem.output, synapse=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim = nengo.Simulator(model3)\n",
    "sim.run(0.2)\n",
    "t = sim.trange()\n",
    "\n",
    "input_net = sim.data[input_probe]\n",
    "output_net = sim.data[output_probe]\n",
    "\n",
    "plot_similarities(input_net, output_net, vocab, top=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the input is similar to `CHERRY` and `APPLE` and somewhat similar to `APRICOT`. As we have specified the threshold of `0.4` the resulting output vector will be similar to `APPLE` and `CHERRY` as their similarity with the input vector is greater than the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hetero-associative relationships\n",
    "\n",
    "To model a variety of interesting memory phenomena, it is often useful to store relationships between different sets of patterns.\n",
    "For example, to simulate number counting from 1 to 5, the memory needs to store the relationships between patterns representing numbers:\n",
    "$1\\rightarrow2,\\ 2\\rightarrow3,\\ 3\\rightarrow4,\\ 4\\rightarrow5$\n",
    "\n",
    "In this example we show how to use `spa.AssociativeMemory` to this task. In order to achieve the number counting, we will split the task into two parts:\n",
    "\n",
    "1. Present a number at the input and recall a number greater by one (e.g. for `1` recall `2`, for `2` recall `3` etc.)\n",
    "    \n",
    "2. Feed the output of the associative memory back to its input\n",
    "\n",
    "    \n",
    "As in the previous example, we start by defining a vocabulary that stores semantic pointes representing five numbers. This will be used as the input and as the output vocabulary. In case of hetero-associative memories the input and the output vocabulary can differ. Because we want to achieve a hetero-associative mapping, we need to specify which input patterns map to which output patterns. The desired mapping in specified by providing `input_keys` and `output_keys`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim = 16\n",
    "vocab_numbers = spa.Vocabulary(dimensions=dim)\n",
    "\n",
    "# a quicker way to add words to a vocabulary\n",
    "vocab_numbers.parse('ONE + TWO + THREE + FOUR + FIVE')\n",
    "\n",
    "# from patterns\n",
    "input_keys = ['ONE', 'TWO', 'THREE', 'FOUR']\n",
    "\n",
    "# to patterns\n",
    "output_keys = ['TWO', 'THREE', 'FOUR', 'FIVE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we introduce a Nengo feature that allows us to pass a function as the input to a `Node`. \n",
    "By passing a function we can specify which semantic pointer is going to be present at the input at certain simulation time point. We choose to present the semantic pointer `ONE` for the similation times between 0 and 0.2, `TWO` for times between 0.2 and 0.4 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def input_fun(t):\n",
    "    if t < 0.2:\n",
    "        return vocab_numbers['ONE'].v\n",
    "    elif t < 0.4:\n",
    "        return vocab_numbers['TWO'].v\n",
    "    elif t < 0.6:\n",
    "        return vocab_numbers['THREE'].v\n",
    "    elif t < 0.8:\n",
    "        return vocab_numbers['FOUR'].v\n",
    "    else:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "with nengo.Network('Counting', seed=1) as model4:\n",
    "    assoc_mem = spa.AssociativeMemory(input_vocab=vocab_numbers, output_vocab=vocab_numbers,\n",
    "                                      input_keys=input_keys, output_keys=output_keys,\n",
    "                                      wta_output=True)\n",
    "    \n",
    "    input_node = nengo.Node(output=input_fun, label='input')\n",
    "    nengo.Connection(input_node, assoc_mem.input)\n",
    "\n",
    "    input_probe = nengo.Probe(input_node)\n",
    "    output_probe = nengo.Probe(assoc_mem.output, synapse=0.03) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim = nengo.Simulator(model4)\n",
    "sim.run(1.)\n",
    "t = sim.trange()\n",
    "\n",
    "input_net = sim.data[input_probe]\n",
    "output_net = sim.data[output_probe]\n",
    "\n",
    "plot_similarities(input_net, output_net, vocab_numbers, top=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have created a model which realises increments by one. The next step is to automatise this, so that when the model sees `ONE` it will produce `TWO, THREE, FOUR, FIVE`.\n",
    "To achieve counting, we need to introduce a feedback connection. That is, the network output needs to be fed into its input at the next time step. This can be easily done in Nengo by adding just one additional connection. Now, we initialise the simulation by presenting the semantic pointer `ONE` at the input for the duration of 0.2 simulation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def input_fun(t):\n",
    "    if 0 < t < 0.2:\n",
    "        return vocab_numbers['ONE'].v\n",
    "    else:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "with nengo.Network('Counting', seed=3) as model5:\n",
    "    assoc_mem = spa.AssociativeMemory(input_vocab=vocab_numbers, output_vocab=vocab_numbers,\n",
    "                                      input_keys=input_keys, output_keys=output_keys, \n",
    "                                      wta_output=True)\n",
    "    \n",
    "    input_node = nengo.Node(output=input_fun, label='input')\n",
    "    nengo.Connection(input_node, assoc_mem.input)\n",
    "\n",
    "    # added feedback connection\n",
    "    nengo.Connection(assoc_mem.output, assoc_mem.input, synapse=.18, transform=3.3)\n",
    "    \n",
    "    input_probe = nengo.Probe(assoc_mem.input)\n",
    "    output_probe = nengo.Probe(assoc_mem.output, synapse=0.03)\n",
    "    \n",
    "sim = nengo.Simulator(model5)\n",
    "sim.run(1.)\n",
    "t = sim.trange()\n",
    "\n",
    "input_net = sim.data[input_probe]\n",
    "output_net = sim.data[output_probe]\n",
    "\n",
    "plot_similarities(input_net, output_net, vocab_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only input our model receives is `ONE` at the beginning of the simulation. After that, it produces a sequence of consecutive numbers up to five."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
